{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2873f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memindai direktori untuk membuat peta lokasi file...\n",
      "Peta lokasi file berhasil dibuat. Ditemukan 549 rekaman unik.\n",
      "DataFrame klinis berhasil dimuat dengan 549 baris.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595b5bdba3c44e85b8f2efa9211887f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mengekstrak Fitur Sinyal:   0%|          | 0/549 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:536: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:536: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:536: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14600\\2311727701.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mfiltered_signal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_signal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_signal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0msignals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mecg_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_signal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m             \u001b[0mhrv_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhrv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\neurokit2\\ecg\\ecg_process.py\u001b[0m in \u001b[0;36mecg_process\u001b[1;34m(ecg_signal, sampling_rate, method, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;31m# Delineate QRS complex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     delineate_signal, delineate_info = ecg_delineate(\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mecg_cleaned\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mecg_cleaned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpeaks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ECG_R_Peaks\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampling_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\neurokit2\\ecg\\ecg_delineate.py\u001b[0m in \u001b[0;36mecg_delineate\u001b[1;34m(ecg_cleaned, rpeaks, sampling_rate, method, show, show_type, check, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m         )\n\u001b[0;32m    179\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"dwt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"discrete wavelet transform\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mwaves\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dwt_ecg_delineator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mecg_cleaned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpeaks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampling_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"prominence\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"peak-prominence\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"emrich\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"emrich2024\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mwaves\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prominence_ecg_delineator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mecg_cleaned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpeaks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrpeaks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msampling_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\neurokit2\\ecg\\ecg_delineate.py\u001b[0m in \u001b[0;36m_dwt_ecg_delineator\u001b[1;34m(ecg, rpeaks, sampling_rate, analysis_sampling_rate)\u001b[0m\n\u001b[0;32m    302\u001b[0m     )\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     tpeaks, ppeaks = _dwt_delineate_tp_peaks(\n\u001b[0m\u001b[0;32m    305\u001b[0m         \u001b[0mecg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrpeaks_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdwtmatr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0manalysis_sampling_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m     )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\neurokit2\\ecg\\ecg_delineate.py\u001b[0m in \u001b[0;36m_dwt_delineate_tp_peaks\u001b[1;34m(ecg, rpeaks, dwtmatr, sampling_rate, qrs_width, p2r_duration, rt_duration, degree_tpeak, degree_ppeak, epsilon_T_weight, epsilon_P_weight)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_P_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdwt_local\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mecg_local\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mecg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrch_idx_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msrch_idx_end\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m         \u001b[0mpeaks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_peaks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdwt_local\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         peaks = list(\n\u001b[0;32m    486\u001b[0m             \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdwt_local\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.025\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdwt_local\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpeaks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import wfdb\n",
    "from scipy import signal\n",
    "import neurokit2 as nk\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==============================================================================\n",
    "# LANGKAH 0: PERSIAPAN DAN PEMBUATAN PETA FILE\n",
    "# ==============================================================================\n",
    "\n",
    "# --- KONFIGURASI ---\n",
    "base_path = r'C:\\Users\\HP\\Documents\\ptbdb'\n",
    "file_path_klinis = os.path.join(base_path, 'metadata_klinis_ekg.csv')\n",
    "\n",
    "# --- MEMBUAT PETA LOKASI FILE (SOLUSI UTAMA) ---\n",
    "print(\"Memindai direktori untuk membuat peta lokasi file...\")\n",
    "file_map = {}\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.hea'):\n",
    "            record_name = filename.split('.')[0]\n",
    "            # Simpan path lengkap TANPA ekstensi file\n",
    "            file_map[record_name] = os.path.join(root, record_name)\n",
    "\n",
    "print(f\"Peta lokasi file berhasil dibuat. Ditemukan {len(file_map)} rekaman unik.\")\n",
    "\n",
    "# Muat DataFrame klinis\n",
    "df_klinis = pd.read_csv(file_path_klinis)\n",
    "print(f\"DataFrame klinis berhasil dimuat dengan {len(df_klinis)} baris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851003ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 1: FUNGSI-FUNGSI PEMBANTU (VERSI BARU)\n",
    "# ==============================================================================\n",
    "\n",
    "def load_ecg_signal_from_map(record_name, file_map):\n",
    "    \"\"\"Fungsi baru yang memuat sinyal menggunakan peta file yang sudah dibuat.\"\"\"\n",
    "    try:\n",
    "        # Langsung ambil path yang benar dari peta\n",
    "        record_path = file_map.get(record_name)\n",
    "        if record_path:\n",
    "            ecg_record = wfdb.rdrecord(record_path)\n",
    "            ecg_signal = ecg_record.p_signal[:, :12]\n",
    "            fs = ecg_record.fs\n",
    "            return ecg_signal, fs\n",
    "        else:\n",
    "            # Jika record tidak ada di peta\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        # print(f\"Error saat membaca record {record_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def filter_signal(signal_data, fs):\n",
    "    \"\"\"Fungsi filter, tidak ada perubahan.\"\"\"\n",
    "    lowcut, highcut = 0.5, 45.0\n",
    "    nyquist = 0.5 * fs\n",
    "    b, a = signal.butter(2, [lowcut / nyquist, highcut / nyquist], btype='band')\n",
    "    return signal.filtfilt(b, a, signal_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 2: EKSTRAKSI FITUR (MENGGUNAKAN FUNGSI BARU)\n",
    "# ==============================================================================\n",
    "\n",
    "list_fitur_sinyal = []\n",
    "gagal_records = []\n",
    "\n",
    "for index, row in tqdm(df_klinis.iterrows(), total=df_klinis.shape[0], desc=\"Mengekstrak Fitur Sinyal\"):\n",
    "    record_name = row['Record']\n",
    "    \n",
    "    # Memanggil fungsi pemuat yang baru dan lebih cerdas\n",
    "    raw_signal, fs = load_ecg_signal_from_map(record_name, file_map)\n",
    "    \n",
    "    if raw_signal is not None and fs is not None:\n",
    "        try:\n",
    "            filtered_signal = filter_signal(raw_signal, fs)\n",
    "            signals, info = nk.ecg_process(filtered_signal[:, 1], sampling_rate=fs)\n",
    "            hrv_features = nk.hrv(info, sampling_rate=fs)\n",
    "            \n",
    "            fitur_saat_ini = {\n",
    "                'Record': record_name,\n",
    "                'Rata_Rata_HR': signals['ECG_Rate'].mean(),\n",
    "                'HRV_SDNN': hrv_features.get('HRV_SDNN', pd.Series([np.nan])).iloc[0],\n",
    "                'HRV_RMSSD': hrv_features.get('HRV_RMSSD', pd.Series([np.nan])).iloc[0]\n",
    "            }\n",
    "            list_fitur_sinyal.append(fitur_saat_ini)\n",
    "        except Exception as e:\n",
    "            # Jika error terjadi saat pemrosesan sinyal (bukan saat memuat)\n",
    "            gagal_records.append(record_name)\n",
    "    else:\n",
    "        # Jika error terjadi saat memuat file\n",
    "        gagal_records.append(record_name)\n",
    "\n",
    "df_fitur_sinyal = pd.DataFrame(list_fitur_sinyal)\n",
    "print(f\"\\n--- Ekstraksi Selesai ---\")\n",
    "print(f\"Berhasil diekstrak: {len(df_fitur_sinyal)} rekaman.\")\n",
    "print(f\"Gagal diekstrak: {len(gagal_records)} rekaman.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2676c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 3: PENGGABUNGAN DAN PEMBERSIHAN\n",
    "# ==============================================================================\n",
    "\n",
    "if not df_fitur_sinyal.empty:\n",
    "    df_lengkap = pd.merge(df_klinis, df_fitur_sinyal, on='Record', how='left')\n",
    "    \n",
    "    # Kita buang baris yang gagal diekstrak fiturnya ATAU yang punya nilai fitur NaN\n",
    "    df_bersih = df_lengkap.dropna(subset=['Rata_Rata_HR', 'HRV_SDNN', 'HRV_RMSSD'])\n",
    "    \n",
    "    print(f\"\\nJumlah data setelah digabung dan dibersihkan: {len(df_bersih)}\")\n",
    "    print(\"\\nDistribusi kelas di DataFrame final yang bersih:\")\n",
    "    print(df_bersih['Diagnosis'].value_counts())\n",
    "    \n",
    "    # Simpan hasil bersih untuk digunakan nanti\n",
    "    final_output_path = os.path.join(base_path, 'dataset_final_untuk_ml.csv')\n",
    "    df_bersih.to_csv(final_output_path, index=False)\n",
    "    print(f\"\\nDataset bersih siap pakai telah disimpan di: {final_output_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nTidak ada fitur yang berhasil diekstrak. Proses dihentikan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c90062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Langkah 0 & 1: Memuat, Grouping, dan Encoding ---\n",
      "Persiapan data awal selesai.\n",
      "\n",
      "--- Langkah 2: Persiapan dan Pemisahan Data ---\n",
      "Pemisahan data berhasil.\n",
      "\n",
      "--- Langkah 2.5: Menangani Nilai NaN dengan Imputasi Median ---\n",
      "Penanganan nilai NaN selesai.\n",
      "\n",
      "--- Langkah 3: Menerapkan SMOTE pada Data Latih ---\n",
      "Proses SMOTE selesai.\n",
      "\n",
      "--- Langkah 4: Melatih Model RandomForestClassifier ---\n",
      "Pelatihan model selesai.\n",
      "\n",
      "--- Langkah 5: Mengevaluasi Model pada Data Uji ---\n",
      "\n",
      "Laporan Klasifikasi:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14600\\2962321980.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;31m# --- PERBAIKAN FINAL DI SINI ---\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;31m# Tambahkan parameter 'labels' untuk memastikan tidak ada kebingungan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m print(classification_report(y_test, y_pred, \n\u001b[0m\u001b[0;32m    103\u001b[0m                             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpected_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                             \u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 4: MEMUAT, GROUPING, DAN ENCODING\n",
    "# ==============================================================================\n",
    "print(\"--- Langkah 0 & 1: Memuat, Grouping, dan Encoding ---\")\n",
    "file_path_final = r\"C:\\Users\\HP\\Documents\\ptbdb\\dataset_final_untuk_ml.csv\" \n",
    "df_final = pd.read_csv(file_path_final)\n",
    "\n",
    "# Grouping Kelas\n",
    "replacement_map = {\n",
    "    'Heart failure (NYHA 2)': 'Heart failure', 'Heart failure (NYHA 3)': 'Heart failure',\n",
    "    'Heart failure (NYHA 4)': 'Heart failure', 'Stable angina': 'Angina/Other Symptoms',\n",
    "    'Unstable angina': 'Angina/Other Symptoms', 'Palpitation': 'Angina/Other Symptoms',\n",
    "    'Hypertrophy': 'Myocardial hypertrophy'\n",
    "}\n",
    "df_final['Diagnosis'] = df_final['Diagnosis'].replace(replacement_map)\n",
    "\n",
    "final_output_path = os.path.join(base_path, 'dataset_final_untuk_ml_final.csv')\n",
    "df_final.to_csv(final_output_path, index=False)\n",
    "\n",
    "# One-Hot Encoding\n",
    "kolom_untuk_encode = df_final.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'Record' in kolom_untuk_encode: kolom_untuk_encode.remove('Record')\n",
    "if 'Diagnosis' in kolom_untuk_encode: kolom_untuk_encode.remove('Diagnosis')\n",
    "df_encoded = pd.get_dummies(df_final, columns=kolom_untuk_encode, dummy_na=False)\n",
    "print(\"Persiapan data awal selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac1f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 5: PERSIAPAN AKHIR & TRAIN-TEST SPLIT\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Langkah 2: Persiapan dan Pemisahan Data ---\")\n",
    "df_encoded['Diagnosis_ID'] = df_encoded['Diagnosis'].astype('category').cat.codes\n",
    "y = df_encoded['Diagnosis_ID']\n",
    "X = df_encoded.drop(columns=['Record', 'Diagnosis', 'Diagnosis_ID'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Pemisahan data berhasil.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe0c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 5.5: IMPUTASI UNTUK MENANGANI NILAI NaN\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Langkah 2.5: Menangani Nilai NaN dengan Imputasi Median ---\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "print(\"Penanganan nilai NaN selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048da71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 6: MENANGANI KETIDAKSEIMBANGAN KELAS (SMOTE)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Langkah 3: Menerapkan SMOTE pada Data Latih ---\")\n",
    "# --- PERBAIKAN FINAL DI SINI ---\n",
    "# Atur k_neighbors menjadi 1, karena kelas terkecil di data latih hanya memiliki 2 anggota (2-1=1).\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(\"Proses SMOTE selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e17a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 7: MELATIH MODEL MACHINE LEARNING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Langkah 4: Melatih Model RandomForestClassifier ---\")\n",
    "model = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"Pelatihan model selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 8 - Mengevaluasi Model pada Data Uji\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Langkah 5: Mengevaluasi Model pada Data Uji ---\")\n",
    "\n",
    "# y_pred seharusnya sudah ada dari Langkah 4 sebelumnya\n",
    "# model juga sudah dilatih\n",
    "\n",
    "# Buat mapping dari ID ke nama diagnosis untuk laporan yang lebih mudah dibaca\n",
    "diagnosis_mapping = dict(enumerate(df_final['Diagnosis'].astype('category').cat.categories))\n",
    "target_names = list(diagnosis_mapping.values())\n",
    "\n",
    "# Definisikan label ID yang kita harapkan (0, 1, 2, ..., 9)\n",
    "expected_labels = list(diagnosis_mapping.keys())\n",
    "\n",
    "print(\"\\nLaporan Klasifikasi:\")\n",
    "# --- PERBAIKAN FINAL DI SINI ---\n",
    "# Tambahkan parameter 'labels' untuk memastikan tidak ada kebingungan\n",
    "print(classification_report(y_test, y_pred, \n",
    "                            labels=expected_labels, \n",
    "                            target_names=target_names, \n",
    "                            zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "# labels juga ditambahkan di sini untuk konsistensi\n",
    "cm = confusion_matrix(y_test, y_pred, labels=expected_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Kelas Sebenarnya (Actual)')\n",
    "plt.xlabel('Kelas Prediksi (Predicted)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2567b707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model berhasil disimpan di lokasi: C:\\Users\\HP\\Documents\\ptbdb\\model_ekg_randomforest.joblib\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 9 - Menyimpan Model\n",
    "# ==============================================================================\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- MENYIMPAN MODEL YANG SUDAH DILATIH ---\n",
    "\n",
    "# 1. Tentukan nama file untuk model Anda\n",
    "#    Konvensi yang baik adalah menggunakan ekstensi .joblib atau .pkl\n",
    "nama_file_model = 'model_ekg_randomforest.joblib'\n",
    "path_model = os.path.join(base_path, nama_file_model) # base_path dari script sebelumnya\n",
    "\n",
    "# 2. Gunakan joblib.dump untuk menyimpan objek 'model' ke dalam file\n",
    "#    'model' adalah nama variabel yang berisi model RandomForest Anda yang sudah dilatih\n",
    "joblib.dump(model, path_model)\n",
    "\n",
    "print(f\"Model berhasil disimpan di lokasi: {path_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee996a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Memuat Model dan Dataset Master ---\n",
      "Model berhasil dimuat.\n",
      "Dataset master berhasil dimuat.\n",
      "\n",
      "--- Mempersiapkan Data Fitur (X) ---\n",
      "Data fitur (X) berhasil dibuat dan dibersihkan.\n",
      "\n",
      "--- Melakukan Prediksi pada Sampel ---\n",
      "\n",
      "--- Hasil Prediksi untuk Pasien 's0053lre' ---\n",
      "Diagnosis Asli Pasien: 'Myocardial infarction'\n",
      "Model Memprediksi Diagnosis: 'Myocardial infarction'\n",
      "Dengan Tingkat Keyakinan: 95.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f66dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 10 - Persiapan & Memuat Aset\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"--- Memuat Model dan Dataset Master ---\")\n",
    "\n",
    "# Definisikan path yang dibutuhkan\n",
    "base_path = r'C:\\Users\\HP\\Documents\\ptbdb' # Sesuaikan jika perlu\n",
    "model_path = os.path.join(base_path, 'model_ekg_randomforest.joblib')\n",
    "data_path = os.path.join(base_path, 'dataset_final_untuk_ml_final.csv')\n",
    "\n",
    "# Muat model yang sudah dilatih\n",
    "model_terlatih = joblib.load(model_path)\n",
    "print(\"Model berhasil dimuat.\")\n",
    "\n",
    "# Muat dataset master yang bersih\n",
    "df_final = pd.read_csv(data_path)\n",
    "print(\"Dataset master berhasil dimuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 11 - Proses Ulang Data Agar Sesuai Format Training\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Mempersiapkan Data Fitur (X) ---\")\n",
    "\n",
    "# 1. One-Hot Encoding\n",
    "kolom_untuk_encode = df_final.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'Record' in kolom_untuk_encode: kolom_untuk_encode.remove('Record')\n",
    "if 'Diagnosis' in kolom_untuk_encode: kolom_untuk_encode.remove('Diagnosis')\n",
    "df_encoded = pd.get_dummies(df_final, columns=kolom_untuk_encode, dummy_na=False)\n",
    "\n",
    "# 2. Definisikan X (kumpulan fitur lengkap)\n",
    "# --- PERBAIKAN DI SINI ---\n",
    "# Kita hanya drop 'Record' dan 'Diagnosis' karena 'Diagnosis_ID' belum ada\n",
    "X_full = df_encoded.drop(columns=['Record', 'Diagnosis'])\n",
    "\n",
    "# 3. Imputasi (Menangani NaN)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_full_imputed = imputer.fit_transform(X_full)\n",
    "X_full_imputed = pd.DataFrame(X_full_imputed, columns=X_full.columns)\n",
    "print(\"Data fitur (X) berhasil dibuat dan dibersihkan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LANGKAH 12 - Ambil Sampel Dan Lakukan Prediksi\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Melakukan Prediksi pada Sampel ---\")\n",
    "# Ambil satu sampel data untuk diuji (misalnya, data pasien ke-50)\n",
    "nomor_sampel = 44\n",
    "sampel_tunggal = X_full_imputed.iloc[[nomor_sampel]]\n",
    "data_asli_pasien = df_final.iloc[nomor_sampel]\n",
    "\n",
    "\n",
    "# Lakukan prediksi dengan model yang sudah dimuat\n",
    "prediksi_id = model_terlatih.predict(sampel_tunggal)\n",
    "prediksi_probabilitas = model_terlatih.predict_proba(sampel_tunggal)\n",
    "\n",
    "# Buat mapping dari ID ke nama diagnosis untuk interpretasi\n",
    "# Kita buat mapping dari data df_final yang asli\n",
    "df_final['Diagnosis_ID'] = df_final['Diagnosis'].astype('category').cat.codes\n",
    "diagnosis_mapping = dict(enumerate(df_final['Diagnosis'].astype('category').cat.categories))\n",
    "nama_diagnosis_prediksi = diagnosis_mapping.get(prediksi_id[0], \"ID Tidak Dikenal\")\n",
    "probabilitas_tertinggi = np.max(prediksi_probabilitas) * 100\n",
    "\n",
    "print(f\"\\n--- Hasil Prediksi untuk Pasien '{data_asli_pasien['Record']}' ---\")\n",
    "print(f\"Diagnosis Asli Pasien: '{data_asli_pasien['Diagnosis']}'\")\n",
    "print(f\"Model Memprediksi Diagnosis: '{nama_diagnosis_prediksi}'\")\n",
    "print(f\"Dengan Tingkat Keyakinan: {probabilitas_tertinggi:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
